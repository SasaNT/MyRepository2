{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2　損失関数\n",
    "\n",
    "　ニューラルネットワークの学習では、損失関数という「ひとつの指標」によって現在の状態を表します。そして、損失関数を基準として最適な重みパラメータの探索を行うのです。なお、損失関数には任意の関数を用いることができますが、一般には、2乗和誤差や交差エントロピー誤差が用いられます。\n",
    " \n",
    "## 4.2.1　2乗和誤差\n",
    "\n",
    "　2乗和誤差は次のように表されます。\n",
    "\n",
    "$$E=\\frac{1}{2}\\displaystyle\\sum_{k}^{}\\left(y_k - t_k\\right)^{2}$$\n",
    "\n",
    "ここで，y<sub>k</sub>はニューラルネットワークの出力，t<sub>k</sub>は教師データを表し，kはデータの次元数を表します。ニューラルネットワークの出力(y<sub>k</sub>)<sub>※</sub>と正解となる教師データ(t<sub>k</sub>)の各要素の差の2乗を計算し、その総和を求めます。\n",
    "<br>※ニューラルネットワークの出力は、ソフトマックス関数で確率と解釈できるものを想定。\n",
    "<br>それでは、Pythonで実装してみましょう。\n",
    " 　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
      "[ 0.1   0.05  0.6   0.    0.05  0.1   0.    0.1   0.    0.  ]\n",
      "(10,)\n",
      "例1：2の確率が高い場合　0.0975\n",
      "例2：7の確率が高い場合　0.5975\n"
     ]
    }
   ],
   "source": [
    "##２乗和誤差を計算する関数を定義\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "##実際に計算してみる。\n",
    "import numpy as np\n",
    "#2を正解とする（教師データ）\n",
    "t = [0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "#例1 :「２」の確率が最も高い場合(0.6)\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "#print(y)\n",
    "#print(np.array(y)) #行列化\n",
    "#print(np.array(y).shape) #10行×1列の行列であることを確認\n",
    "mean_squared_error(np.array(y),np.array(t))\n",
    "print(\"例1：2の確率が高い場合　\" + str(mean_squared_error(np.array(y),np.array(t))))\n",
    "\n",
    "#例2 :「７」の確率が最も高い場合(0.6)\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "mean_squared_error(np.array(y),np.array(t))\n",
    "print(\"例2：7の確率が高い場合　\" + str(mean_squared_error(np.array(y),np.array(t))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 例1は、教師データと正解が同じ(2)であり、損失関数(2乗和誤差)の値は0.097です。一方、例2は教師データと正解が異なり(7)、損失関数の値は0.5975と例１に比べて高くなります。このように正解データと一致する場合、誤差が小さくなることが分かります。つまり、例１の方が教師データにより適合していることを２乗和誤差は示しているのです。\n",
    "\n",
    "## 4.2.2　交差エントロピー誤差\n",
    "\n",
    "　交差エントロピー誤差は次のように表されます。\n",
    "\n",
    "$$E=-\\displaystyle\\sum_{k}^{}t_k\\log y_k$$\n",
    "\n",
    "　ここで、logは底がeの自然対数(log<sub>e</sub>)を表します。y<sub>k</sub>はニューラルネットワークの出力、t<sub>k</sub>はone-hot表現の正解ラベルとします。そのため、交差エントロピー誤差は実質的に正解ラベルが1に対応する出力の自然対数を計算するだけになります。\n",
    " <br>例えば、「２」が正解ラベルのインデックスがあるとして、それに対応するニューラルネットワークの出力が0.6の場合、交差エントロピー誤差は、-log0.6 = 0.51となります。\n",
    " ところで自然対数をグラフで表すと、以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satoshi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAFkCAYAAABB1xPiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEklGRQkSVJJBgVJklTS/weJ11w1Z6ZLCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x45baada8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.xscale(\"log\")\n",
    "x = np.arange(0.0,1,0.01,\"float\")\n",
    "y = np.log(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　上記図のように、自然対数はxが「1」のときに、yは「0」になり、xが「0」に近づくにつれてyの値はどんどん小さくなります。そのため交差エントロピー誤差は、正解ラベルに対するニューラルネットの出力(確率)が高いほど0に近づきます。そして、正解ラベルに対する出力が小さい場合、値が大きくなることもわかります。(マイナスを乗じているので)\n",
    "<br>それでは、交差エントロピー誤差を実装しましょう。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例1：2の確率が高い場合　0.510825457099\n",
      "例2：7の確率が高い場合　2.30258409299\n"
     ]
    }
   ],
   "source": [
    "##交差エントロピー誤差を計算する関数を定義\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 #マイナス無限大対策\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "##実際に計算してみる。\n",
    "import numpy as np\n",
    "#2を正解とする（教師データ）\n",
    "t = [0,0,1,0,0,0,0,0,0,0] \n",
    "\n",
    "#例1 :「２」の確率が最も高い場合(0.6)\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "cross_entropy_error(np.array(y),np.array(t))\n",
    "print(\"例1：2の確率が高い場合　\" + str(cross_entropy_error(np.array(y),np.array(t))))\n",
    "\n",
    "#例2 :「７」の確率が最も高い場合(0.6)\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "cross_entropy_error(np.array(y),np.array(t))\n",
    "print(\"例2：7の確率が高い場合　\" + str(cross_entropy_error(np.array(y),np.array(t))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 例１(正解と出力が一致)は、正解ラベルと一致するニューラルネットの出力(確率)が0.6で、このときの交差エントロピー誤差は約0.51です。一方、例2(正解と出力が不一致)の場合は、正解ラベルと一致するニューラルネットの出力が0.1で、交差エントロピー誤差は約2.3と高くなることがわかります。\n",
    "\n",
    "<br>※交差エントロピー誤差関数では、微小な値である delta を足して計算しています。これは、np.log(0) の ような計算が発生した場合、np.log(0) はマイナスの無限大を表す-inf となり、そうなってしまうと、それ以上計算を進めることができなくなります。その防止策として、微小な値を追加して、マイナス無限大を発生させないようにしています。\n",
    "\n",
    "## 4.2.3　ミニバッチ学習\n",
    "\n",
    "　機械学習の問題は、訓練データに対する損失関数を求め、その値をできるだけ小さくするようなパラメータを探し出すということです。そのため訓練データが100個あれば、その100個の損失関数の和を指標とします。例えば、交差エントロピー誤差ですべての損失関数の和を求める場合、次のようになります。\n",
    " \n",
    " $$E=-\\frac{1}{N}\\displaystyle\\sum_{n}^{}\\displaystyle\\sum_{k}^{}t_{nk}\\log y_{nk}$$\n",
    "\n",
    " これは、ひとつのデータに対する交差エントロピー誤差を求める式を、単にＮ個分のデータに拡張しただけです。最後にＮ個で割って正規化(平均化)しています。こうしておくことで、訓練データが大量になった場合でも統一した指標で比較できます。\n",
    "\n",
    " ところで、MINISTデータは訓練データが60,000個と大量にあり、処理速度に影響がでる可能性があります（ましてやビッグデータを扱うなら・・・）。そこで、訓練データからある枚数だけを選び出し(ミニバッチ)、ミニバッチごとに学習をおこないます。これをミニバッチ学習と言います。\n",
    "<br>それでは、訓練データから指定された個数のデータ(ミニバッチ)を、ランダムに取り出すコードを書いてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "[41925 43059 10195 23415 53114 23191 49122 56662 35963 52752]\n",
      "(10, 784)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "#ご自身のPATHを指定してください。\n",
    "path = \"C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\deep-learning-from-scratch-master\\\\ch01\"\n",
    "\n",
    "#各種import\n",
    "import sys, os\n",
    "import numpy as np\n",
    "os.chdir(path)\n",
    "sys.path.append(os.pardir)\n",
    "#特にmnist関数をインポート\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "#MNISTデータセットの読み込み。最初の呼び出しは数分待ちます\n",
    "(x_train, t_train),(x_test,t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "#訓練データが60,000個あり、入力データが784次元(28×28)であることの確認\n",
    "print(x_train.shape)\n",
    "#教師データが10次元データであることの確認\n",
    "print(t_train.shape)\n",
    "\n",
    "##訓練データからランダムに10枚抜き出す処理（ミニバッチ）\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "#ミニバッチとして取り出すインデックスを確認\n",
    "print(batch_mask)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "#ミニバッチが取り出されていることを確認\n",
    "print(x_batch.shape)\n",
    "print(t_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4　【バッチ対応版】交差エントロピー誤差の実装\n",
    "\n",
    "　では、ミニバッチに対応した交差エントロピー誤差を実装してみましょう。\n",
    "　例は、教師データがone-hot表現で与えられる場合です。one-hot以外の場合は教科書を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[[ 0.86094853  0.84872725  0.79110789  0.40970111  0.94098277  0.0567429\n",
      "   0.9904154   0.95205925  0.58975359  0.60197176]\n",
      " [ 0.28022456  0.19519843  0.33346739  0.26898108  0.07615626  0.3992573\n",
      "   0.86480462  0.53963582  0.56948716  0.72549039]\n",
      " [ 0.00344016  0.3980288   0.53596307  0.09127167  0.93562066  0.26263358\n",
      "   0.62039035  0.12055518  0.3310635   0.01588555]\n",
      " [ 0.63838081  0.81823976  0.40962994  0.20926959  0.66635333  0.94983957\n",
      "   0.26547824  0.3345301   0.90413152  0.14510172]\n",
      " [ 0.56672855  0.19835813  0.89809675  0.27652551  0.31179973  0.13134176\n",
      "   0.72745473  0.4702393   0.42178454  0.77067559]\n",
      " [ 0.95179222  0.72958605  0.03078512  0.81818841  0.13521398  0.13011484\n",
      "   0.50253071  0.85723353  0.96857217  0.2753736 ]\n",
      " [ 0.22405289  0.2952663   0.42082453  0.05491143  0.80686676  0.24501461\n",
      "   0.14049472  0.18943616  0.09114968  0.15785935]\n",
      " [ 0.81132115  0.72988619  0.71525787  0.44835119  0.1259876   0.92648274\n",
      "   0.1262667   0.17097819  0.85240194  0.47198942]\n",
      " [ 0.56606921  0.73157054  0.79292524  0.531309    0.70883988  0.13471657\n",
      "   0.75611771  0.82620998  0.46718829  0.65802694]\n",
      " [ 0.13856295  0.54733664  0.01402855  0.89299167  0.35267024  0.15204525\n",
      "   0.57553433  0.47325735  0.31937726  0.06163095]]\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "10\n",
      "1.68289896662\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 #マイナス無限大対策\n",
    "    \n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    print(batch_size)\n",
    "    return -np.sum(t * np.log(y + delta)) / batch_size\n",
    "\n",
    "##上記のコードで設定済みのバッチ数10を使って実行してみる\n",
    "y_batch = np.empty((0,10))\n",
    "print(y_batch)\n",
    "for i in range(10):\n",
    "    dm_batch = np.array(np.random.rand(10))\n",
    "    #a = np.r_[a,b.reshape(1,-1)]\n",
    "    y_batch = np.r_[y_batch , dm_batch.reshape(1,-1)]\n",
    "    \n",
    "print(y_batch)\n",
    "print(t_batch)\n",
    "#y_batch = \n",
    "result = cross_entropy_error(y_batch,t_batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2.5　なぜ損失関数を設定するのか\n",
    "\n",
    "わざわざ「損失関数」を導入せずとも、「認識精度」そのものを指標にすればよいのではないか？という疑問があるかもしれません。\n",
    "\n",
    "ニューラルネットワークの学習では、最適なパラメータ(重みとバイアス)を探索する際に、損失関数の値ができるだけ小さくなるようなパラメータを探しますが、この作業は損失関数を微分することで行っています（詳細は次節）。認識精度を指標にすると、パラメータの微分がほとんどの場所で0になってしまい、パラメータを探す作業が行えないです<sub>※</sub>。認識精度はパラメータの微小な変化にほとんど反応を示さず、もし反応があっても値が不連続に変化するためです。したがって、損失関数という指標を用います。\n",
    "\n",
    "<br>※例えば、あるニューラルネットワークが現在100枚ある訓練データの中で32枚正しく認識できているとします(認識率32%)。このときに重みパラメータの値を少し変えただけでは、、認識精度の32%は変化しないでしょう。つまり、パラメータの少しの調整だけでは、認識精度は改善されず一定のままなのです。仮に、改善されたとしても、32.0123…のような連続的な変化ではなく33%、34%のように不連続な値となるでしょう。一方、損失関数を指標とした場合、現在の損失関数の値が092543…のような場合、パラメータ少しの変化で0.93432…のように連続的に変化するのです。\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
