{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.6 Affine / Softmaxレイヤの実装\n",
    "\n",
    "### 5.6.1 Affineレイヤ ～ 5.6.2 バッチ版Affineレイヤの復習\n",
    "<font size=4>■Affineとは？</font>\n",
    "<br>ニューラルネットワークの順伝播で行う行列の内積処理は、幾何学の分野では「アフィン変換」と呼ばれる。\n",
    "<br>このため、本書ではアフィン変換を行う処理を「Affineレイヤ」という名前で呼び、実装します。\n",
    "\n",
    "<font size=4>■ニューラルネットの内積処理（順伝播の流れ）とは？</font>\n",
    "<br>以下のような処理を行うこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[ 0.40793469  0.49687236]\n",
      "W:\n",
      "[[ 0.24357109  0.32002027  0.6062862 ]\n",
      " [ 0.21971543  0.1130781   0.24374492]]\n",
      "B:\n",
      "[ 0.8457568   0.20431532  0.42363894]\n",
      "Y:\n",
      "[ 1.05428842  0.39104807  0.79207423]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#入力\n",
    "X = np.random.rand(2) #1行2列の行列（ベクトル）を作る\n",
    "#重み\n",
    "W = np.random.rand(2,3)#2行3列の行列を作る\n",
    "#バイアス\n",
    "B = np.random.rand(3) #1行3列の行列（ベクトル）を作る\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print(\"W:\")\n",
    "print(W)\n",
    "print(\"B:\")\n",
    "print(B)\n",
    "\n",
    "#XとWの内積を行い、バイアスを加算\n",
    "##イメージはP58 図3-14にバイアスを加算したもの\n",
    "Y = np.dot(X,W) + B\n",
    "print(\"Y:\")\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このとき注意すべきことは、行列の対応する次元数を一致させることです。\n",
    "<br><br>\n",
    "![5-18内積注意点](\".\\img\\5-18内積注意点.png\")\n",
    "<img src = \".\\img\\5-18内積注意点.png\" width=\"400\" height=\"300\">\n",
    "<br><br>\n",
    "これまでの順伝播（内積処理）を計算グラフで記載すると。\n",
    "<br>なお、各変数の上部に変数の形状も表記。\n",
    "<br><br>\n",
    "![5-19Affineレイヤ計算グラフ](\".\\img\\5-19Affineレイヤ計算グラフ.png\")\n",
    "<img src = \".\\img\\5-19Affineレイヤ計算グラフ.png\" width=\"400\" height=\"300\">\n",
    "<br><br>\n",
    "これまでの計算グラフは、「スカラ値」がノード間をながれていましたが、本例では「行列」がノード間を伝播しています。\n",
    "<br><br>\n",
    "<font size=4>■Affineの逆伝播処理</font>\n",
    "<br><br>\n",
    "![5-20Affineレイヤ逆伝播](\".\\img\\5-20Affineレイヤ逆伝播.png\")\n",
    "<img src = \".\\img\\5-20Affineレイヤ逆伝播.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "書籍ではいきなり\n",
    "$$\\frac{ \\partial L }{ \\partial X } = \\frac{ \\partial L }{ \\partial Y } ・ W^{ \\mathrm{ T } }$$\n",
    "$$\\frac{ \\partial L }{ \\partial W } = X^{ \\mathrm{ T } } ・ \\frac{ \\partial L }{ \\partial Y } $$\n",
    "と出てくる。\n",
    "<br>これは意味不明だと思うので、次の例で確かめてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "![5-22Affineレイヤ逆伝播お勉強](\".\\img\\5-22Affineレイヤ逆伝播お勉強.png\")\n",
    "<img src = \".\\img\\5-22Affineレイヤ逆伝播お勉強.png\" width=\"500\" height=\"400\">\n",
    "<br><br>\n",
    "\n",
    "入力として$x=(x_1,x_2)$の２次元だとします。\n",
    "この入力に対して、第１層目への出力を３つにしたい場合、(2,3)の行列を右からかけます。つまり重みWを\n",
    "$$W = \\begin{pmatrix} w_{11} & w_{21} & w_{31} \\\\ w_{12} & w_{22} & w_{32} \\end{pmatrix}$$\n",
    "とします。つまり、出力Ｙは、\n",
    "$$Y = X・W$$\n",
    "$$  = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} ・ \\begin{pmatrix} w_{11} & w_{21} & w_{31} \\\\ w_{12} & w_{22} & w_{32} \\end{pmatrix}$$\n",
    "$$  = \\begin{pmatrix} w_{11}・x_1+w_{12}・x_2 & w_{21}・x_1+w_{22}・x_2 & w_{31}・x_1+w_{32}・x_2 \\end{pmatrix} $$\n",
    "$$  = \\begin{pmatrix} y_1 & y_2 & y_3 \\end{pmatrix} $$\n",
    "となります。\n",
    "<br>損失関数LLの入力Xによる偏微分は$x_1,x_2がy_1,y_2,y_3$に出てくることに注意すると、\n",
    "$$\\frac{ \\partial L }{ \\partial X } = \\begin{pmatrix} \\frac{ \\partial L }{ \\partial x_1 } & \\frac{ \\partial L }{ \\partial x_2 } \\end{pmatrix}$$\n",
    "$$= \\begin{pmatrix} \\frac{ \\partial L }{ \\partial Y }・\\frac{ \\partial Y }{ \\partial x_1 } & \\frac{ \\partial L }{ \\partial Y }・\\frac{ \\partial Y }{ \\partial x_2 } \\end{pmatrix}$$\n",
    "ここで、\n",
    "$$\\frac{ \\partial L }{ \\partial Y }・\\frac{ \\partial Y }{ \\partial x_1 }  = \\begin{pmatrix} \\frac{ \\partial L }{ \\partial y_1 } & \\frac{ \\partial L }{ \\partial y_2 } & \\frac{ \\partial L }{ \\partial y_2 } \\end{pmatrix}・\\begin{pmatrix} \\frac{ \\partial y_1 }{ \\partial x_1 } \\\\ \\frac{ \\partial y_2 }{ \\partial x_2 } \\\\ \\frac{ \\partial y_3 }{ \\partial x_3 } \\end{pmatrix}$$\n",
    "なので、\n",
    "$$\\frac{ \\partial L }{ \\partial X } =  \\begin{pmatrix} \\frac{ \\partial L }{ \\partial y_1 }・\\frac{ \\partial y_1 }{ \\partial x_1 }+\\frac{ \\partial L }{ \\partial y_2 }・\\frac{ \\partial y_2 }{ \\partial x_1 } + \\frac{ \\partial L }{ \\partial y_3 }・\\frac{ \\partial y_3 }{ \\partial x_1 } & \\frac{ \\partial L }{ \\partial y_1 }・\\frac{ \\partial y_1 }{ \\partial x_2 }+\\frac{ \\partial L }{ \\partial y_2 }・\\frac{ \\partial y_2 }{ \\partial x_2 } + \\frac{ \\partial L }{ \\partial y_3 }・\\frac{ \\partial y_3 }{ \\partial x_2 } \\end{pmatrix}$$\n",
    "$$ = \\begin{pmatrix} \\frac{ \\partial L }{ \\partial y_1 }・w_{11} + \\frac{ \\partial L }{ \\partial y_2 }・w_{21} + \\frac{ \\partial L }{ \\partial y_3 }・w_{31} & \\frac{ \\partial L }{ \\partial y_1 }・w_{12} + \\frac{ \\partial L }{ \\partial y_2 }・w_{22} + \\frac{ \\partial L }{ \\partial y_3 }・w_{32} \\end{pmatrix}$$\n",
    "$$ = \\begin{pmatrix} \\frac{ \\partial L }{ \\partial y_1 } & \\frac{ \\partial L }{ \\partial y_2 } & \\frac{ \\partial L }{ \\partial y_2 } \\end{pmatrix}・ \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \\end{pmatrix}$$\n",
    "$$ = \\frac{ \\partial L }{ \\partial Y }・W^{ \\mathrm{ T } }$$\n",
    "<br>\n",
    "<br>※参考サイト\n",
    "<br>Affineレイヤの逆伝播を地道に成分計算する\n",
    "<br>https://qiita.com/yuyasat/items/d9cdd4401221df5375b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.6.2 バッチ版Affineレイヤの実装\n",
    "\n",
    "これまでの説明は入力であるXは一つのデータ（ベクトル）を対象としたものでしたが、ここではバッチ版のAffineレイヤとしてＮ個のデータをまとめて（行列化して）順伝播する方法を考えます。バッチ版のAffineレイヤは次のようになります。\n",
    "<br><br>\n",
    "![5-21バッチ版Affineレイヤ](\".\\img\\5-21バッチ版Affineレイヤ.png\")\n",
    "<img src = \".\\img\\5-21バッチ版Affineレイヤ.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "先ほどまでと異なる点は、入力であるXの形状が(N, 2)になっただけです。後は、先ほどまでと同様に計算グラフ上で計算することができます。また、逆伝播も行列の形状にだけ注意すれば、$\\frac{ \\partial L }{ \\partial X }$、$\\frac{ \\partial L }{ \\partial W }$も同様に導出できます。\n",
    "<br>ただし、バイアスの計算は注意が必要です。順伝播の加算はバイアスがそれぞれのデータに加算されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_dot_w\n",
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "x_dot_w + b\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_dot_w = np.array([[0,0,0],[10,10,10]])\n",
    "b = np.array([1,2,3])\n",
    "print(\"x_dot_w\")\n",
    "print(x_dot_w)\n",
    "print(\"x_dot_w + b\")\n",
    "print(x_dot_w + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dot_W:\n",
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "B:\n",
      "[1 2 3]\n",
      "X_dot_W + B:\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#入力\n",
    "X_dot_W = np.array([[0,0,0],[10,10,10]]) \n",
    "#バイアス\n",
    "B = np.array([1,2,3]) \n",
    "print(\"X_dot_W:\")\n",
    "print(X_dot_W)\n",
    "print(\"B:\")\n",
    "print(B)\n",
    "print(\"X_dot_W + B:\")\n",
    "print(X_dot_W + B)\n",
    "\n",
    "#XとWの内積を行い、バイアスを加算\n",
    "#Y = np.dot(X,W) + B\n",
    "#print(\"Y:\")\n",
    "#print(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このため、逆伝播の際には、それぞれのデータの逆伝播の値がバイアスの要素に集約される必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dY\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "dB\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "print(\"dY\")\n",
    "print(dY)\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(\"dB\")\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例では、データが2個（N=2)あるものと仮定します。バイアスの逆伝播は、<b>その2個のデータに対しての微分を、データごとに合算して求めます(？)。</b><b>そのため、np.sum()で0番目の軸に対して総和を求めるのです(？)。</b>以上からAffineレイヤの実装は以下の通りとなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Class Affine:\n",
    "    #インスタンス生成時の処理\n",
    "    def __init__(self, W ,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "    \n",
    "    #順伝播\n",
    "    def forward(self,x):\n",
    "        self.x = x #自身のXにXを代入\n",
    "        out = np.dot(x, self.W) + self.b #内積計算+バイアス\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    #逆伝播\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) #X側の逆伝播を求める dout:前の層から逆伝播されてきた値と重みWの転置行列の内積\n",
    "        self.dw = np.dot(self.x.T, dout) #W側の逆伝播を求める dout:前の層から逆伝播されてきた値と入力値Xの転置行列の内積\n",
    "        self.db = np.sum(dout, axis=0) #データごとに合計していることは分るが、なんで逆伝播がこれでOKかは不明。。\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3　Softmax-with-Lossレイヤ\n",
    "最後に、出力層であるソフトマックス関数について説明します。ソフトマックス関数は、入力された値を正規化して出力します。\n",
    "<br><br>\n",
    "![5-23SoftMax例.png](\".\\img\\5-23SoftMax例.png\")\n",
    "<img src = \".\\img\\5-23SoftMax例.png\" width=\"500\" height=\"400\">\n",
    "<br><br>\n",
    "本書では、Softmaxレイヤの実装に交差エントロピー誤差も含めて「Softmax-With-Lossレイヤ」という名前で実装しています。\n",
    "それでは、このレイヤの計算グラフを示します。\n",
    "<br><br>\n",
    "![5-24SoftMax-With-Lossレイヤ.png](\".\\img\\5-24SoftMax-With-Lossレイヤ.png\")\n",
    "<img src = \".\\img\\5-24SoftMax-With-Lossレイヤ.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax-With-Lossレイヤの計算グラフ\n",
    "### ■順伝播\n",
    "まずは、Softmaxレイヤです。Softmax関数は以下の数式で表されます。\n",
    "$$  y_k = \\frac{ \\exp ( a_k ) }{ \\displaystyle \\sum_{ i = 1 }^{ n } \\exp ( a_i ) } $$\n",
    "これを計算グラフで表すと以下のようになります。\n",
    "<br><br>\n",
    "![5-26SoftMaxレイヤ順伝播.png](\".\\img\\5-26SoftMaxレイヤ順伝播.png\")\n",
    "<img src = \".\\img\\5-26SoftMaxレイヤ順伝播.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "続いて、Cross Entropy Errorです。Cross Entropy Errorは数式で次のようにあらわされます。\n",
    "$$ L = -\\sum_{k}^{ } t_k \\log  y_k   $$\n",
    "これを計算グラフで表すと以下のようになります。\n",
    "<br><br>\n",
    "![5-27CrossEntropyErrorレイヤ順伝播.png](\".\\img\\5-27CrossEntropyErrorレイヤ順伝播.png)\n",
    "<img src = \".\\img\\5-27CrossEntropyErrorレイヤ順伝播.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "### ■逆伝播\n",
    "まずは、Cross Entropy Errorの逆伝播を考えます。\n",
    "以下のポイントを抑えれば、逆伝播は簡単です。\n",
    "#### 1.『×』ノードの逆伝播は、順伝播時の入力値をひっくり返した値を、上流からの微分に乗算して下流に流す\n",
    "#### 2.『＋』ノードでは、上流からの微分をそのまま流す\n",
    "#### 3.『log』の逆伝播は次の式に従う\n",
    "\n",
    "$$ y = \\log x$$\n",
    "$$ \\frac{ \\partial y }{ \\partial x } = \\frac{ 1 }{ x }$$ \n",
    "計算グラフで表すと以下のようになります。\n",
    "<br><br>\n",
    "![5-28CrossEntropyErrorレイヤ逆伝播.png](\".\\img\\5-28CrossEntropyErrorレイヤ逆伝播.png)\n",
    "<img src = \".\\img\\5-28CrossEntropyErrorレイヤ逆伝播.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "次に、Softmaxレイヤの逆伝播を考えます。複雑なので、一つずつ確認しながら考えます。\n",
    "<br><br>\n",
    "![5-29SoftMaxレイヤ逆伝播1.png](\".\\img\\5-29SoftMaxレイヤ逆伝播1.png)\n",
    "<img src = \".\\img\\5-29SoftMaxレイヤ逆伝播1.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "まず、前レイヤ（Cross Entropy Errorレイヤ）から逆伝播の値が流れてきます。\n",
    "<br><br>\n",
    "![5-30SoftMaxレイヤ逆伝播2.png](\".\\img\\5-30SoftMaxレイヤ逆伝播2.png)\n",
    "<img src = \".\\img\\5-30SoftMaxレイヤ逆伝播2.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "「×」ノードでは順伝播の値をひっくり返して乗算します。ここでは次の計算が行われます。\n",
    "$$ - \\frac{ t_1 }{ y_1 } \\exp(a_1) = -t_1 \\frac{ S }{ \\exp(a_1) } \\exp(a_1) = -t_1 S $$\n",
    "<br><br>\n",
    "![5-31SoftMaxレイヤ逆伝播3.png](\".\\img\\5-31SoftMaxレイヤ逆伝播3.png)\n",
    "<img src = \".\\img\\5-31SoftMaxレイヤ逆伝播3.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "順伝播で枝分かれした場合、逆伝播ではそれらの値が加算されます。つまり$(-t_1S,-t_2S,-t_3S)$が加算されます。そしてその加算された値に「/」の逆伝播を行うので、結果は1/s(t_1+t_2+t_3)となります（？）。またここでtはOne-hot表現で対応する値は1でそれ以外は0であるので、t_1～t_3の和は1となります。\n",
    "<br><br>\n",
    "![5-32SoftMaxレイヤ逆伝播4.png](\".\\img\\5-32SoftMaxレイヤ逆伝播4.png)\n",
    "<img src = \".\\img\\5-32SoftMaxレイヤ逆伝播4.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "「＋」ノードは流すだけ。\n",
    "<br><br>\n",
    "![5-33SoftMaxレイヤ逆伝播5.png](\".\\img\\5-33SoftMaxレイヤ逆伝播5.png)\n",
    "<img src = \".\\img\\5-33SoftMaxレイヤ逆伝播5.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "「×」ノードはひっくり返して乗算。\n",
    "<br><br>\n",
    "![5-34SoftMaxレイヤ逆伝播6.png](\".\\img\\5-34SoftMaxレイヤ逆伝播6.png)\n",
    "<img src = \".\\img\\5-34SoftMaxレイヤ逆伝播6.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の計算グラフを簡略化して書くと以下のようにできます。\n",
    "<br><br>\n",
    "![5-25SoftMax-With-Lossレイヤ簡略版.png](\".\\img\\5-25SoftMax-With-Lossレイヤ簡略版.png\")\n",
    "<img src = \".\\img\\5-25SoftMax-With-Lossレイヤ簡略版.png\" width=\"600\" height=\"500\">\n",
    "<br><br>\n",
    "ここでは、3クラス分類を行う場合を想定し、前レイヤから３つの入力（スコア）を受け取るものとします。Softmaxレイヤは、入力である$\\begin{pmatrix} a_1 & a_2 & a_3 \\end{pmatrix}$を正規化して、$\\begin{pmatrix} y_1 & y_2 & y_3 \\end{pmatrix}$を出力します。Corss Entropy Errorレイヤは、Softmaxの出力$\\begin{pmatrix} y_1 & y_2 & y_3 \\end{pmatrix}$と、教師ラベルの$\\begin{pmatrix} t_1 & t_2 & t_3 \\end{pmatrix}$を受け取り、それらのデータから損失Lを出力します。\n",
    "<br>ここで注目すべきは、逆伝播の結果です。Softmaxレイヤからの逆伝播は、$\\begin{pmatrix} y_1 - t_1 & y_2 - t_2 & y_3  - t_3\\end{pmatrix}$というSoftmaxレイヤの出力と教師データの差分という<b>『キレイ』</b>な結果になっています。ニューラルネットワークの逆伝播では、この差分である誤差が前レイヤに伝わっていくのです。<b>この性質はニューラルネットの学習における重要な性質です。</b>\n",
    "<br>このようなキレイな結果が返ってくるのは偶然でしょうか？これは偶然ではなく、このような結果が返ってくるように設計された関数が交差エントロピー誤差なのです。また、回帰問題では出力層に「恒等関数」を用いて、損失関数「2乗和誤差」を用いますが、これも同様の理由です。つまり、恒等関数の損失関数として2常和誤差を用いると、逆伝播が$\\begin{pmatrix} y_1 - t_1 & y_2 - t_2 & y_3  - t_3\\end{pmatrix}$というキレイな結果になります。\n",
    "<br><br>\n",
    "ここで具体例を考えてみましょう。\n",
    "<br>例えば教師ラベルが$\\begin{pmatrix} 0 & 1 & 0\\end{pmatrix}$であるデータに対して、Softmaxレイヤの出力が$\\begin{pmatrix} 0.3 & 0.2 & 0.5\\end{pmatrix}$であった場合を考えます。正解ラベルに対する確率は20%(0.2)なので、この時点のニューラルネットワークは正しい認識ができていません。この場合、Softmaxレイヤからの逆伝播は、$\\begin{pmatrix} 0.3 & -0.8 & 0.5\\end{pmatrix}$という大きな誤差を伝播することとなりますので、前のレイヤではその大きな誤差から大きな内容を学習することになります。\n",
    "<br>また、別の例として、教師ラベルが$\\begin{pmatrix} 0 & 1 & 0\\end{pmatrix}$であるデータに対して、Softmaxレイヤの出力が$\\begin{pmatrix} 0.01 & 0.99 & 0\\end{pmatrix}$であった場合を考えます。この場合Softmaxレイヤからの逆伝播の値は$\\begin{pmatrix} 0.01 & -0.01 & 0\\end{pmatrix}$という非常に小さい誤差になります。そのため、Softmaxレイヤより前のレイヤでは学習する内容も小さくなります。\n",
    "それでは、Softmax with　Lossレイヤを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def ___init__(self):\n",
    "        self.loss = None #損失\n",
    "        self.y = None #Softmaxの出力\n",
    "        self.t = None #教師データ(One-hot表現)\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        #過去に作ったSoftmax関数とCross Entropy Error関数を利用\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "        \n",
    "    def backward(self, dout=1):\n",
    "        #入力されたデータの行数（バッチの個数）を取得\n",
    "        batch_size = self.shape[0]\n",
    "        #バッチの個数で割ることで、データ1個当たりの誤差が前レイヤに伝播するようにしている（？）\n",
    "        dx = (self.y - self.t) / bitch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################　順伝播　###################\n",
      "X:\n",
      "[ 0.90371264  0.49823441]\n",
      "W:\n",
      "[[ 0.51374329  0.13308606  0.48219582]\n",
      " [ 0.60324287  0.69663883  0.82085857]]\n",
      "B:\n",
      "[ 0.10743976  0.15812806  0.96268844]\n",
      "Y:\n",
      "[ 0.87227242  0.62548905  1.80743489]\n",
      "#####################　逆伝播　###################\n",
      "X_t:\n",
      "[[ 0.90371264]\n",
      " [ 0.49823441]]\n",
      "W_t:\n",
      "[[ 0.51374329  0.60324287]\n",
      " [ 0.13308606  0.69663883]\n",
      " [ 0.48219582  0.82085857]]\n",
      "X:\n",
      "[ 1.12902517  2.12074027]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"#####################　順伝播　###################\")\n",
    "#入力\n",
    "X = np.random.rand(2) #1行2列の行列（ベクトル）を作る\n",
    "#重み\n",
    "W = np.random.rand(2,3) #2行3列の行列を作る\n",
    "#バイアス\n",
    "B = np.random.rand(3) #1行3列の行列（ベクトル）を作る\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print(\"W:\")\n",
    "print(W)\n",
    "print(\"B:\")\n",
    "print(B)\n",
    "\n",
    "#XとWの内積を行い、バイアスを加算\n",
    "Y = np.dot(X,W) + B\n",
    "print(\"Y:\")\n",
    "print(Y)\n",
    "    \n",
    "print(\"#####################　逆伝播　###################\")  \n",
    "Y_Bak = np.array([1,1,1])\n",
    "##転置\n",
    "X_t = X.reshape(-1,1) #1次元は普通に転置できないのでreshapeを使う。(-1,1)を指定することで縦横ベクトルを入れ替える\n",
    "W_t = W.T #2次元以上の配列は、配列.Tで転置可能\n",
    "print(\"X_t:\")\n",
    "print(X_t)\n",
    "print(\"W_t:\")\n",
    "print(W_t)\n",
    "\n",
    "##内積\n",
    "result_dotX = np.dot(Y_Bak,W_t)\n",
    "#result_dotW = np.dot(X_t,Y)\n",
    "print(\"X:\")\n",
    "print(result_dotX)\n",
    "#print(\"W:\")\n",
    "#print(result_dotW)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
