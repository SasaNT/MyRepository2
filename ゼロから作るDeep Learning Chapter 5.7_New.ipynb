{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 誤差逆伝播法の実装\n",
    "\n",
    "### 5.7.1 ニューラルネットワークの学習の全体像\n",
    "\n",
    "ニューラルネットワークの学習とは、重みとバイアスを訓練データに適応することであり、次の4つのステップで行う。\n",
    "\n",
    "#### ステップ1（ミニバッチ）\n",
    "　訓練データの中からランダムに一部のデータを選び出す。\n",
    " \n",
    "#### ステップ2（勾配の算出）\n",
    "　各重みパラメータに関する損失関数の勾配を求める。\n",
    "\n",
    "#### ステップ3（パラメータの更新）\n",
    "　重みパラメータを勾配方向に微笑量だけ更新する。\n",
    "\n",
    "#### ステップ4（繰り返す=ミニバッチサイズ分まで）\n",
    "　ステップ1、ステップ2、ステップ3を繰り返す。\n",
    " \n",
    "#### ステップ5（繰り返す=ステップ1~4=epoh）\n",
    "　訓練データの中からランダムに一部のデータを選び出す。\n",
    " \n",
    "前節までは、ステップ2の勾配を求めるために、数値微分を用いていたが、計算時間がかかる。\n",
    "<br>誤差逆伝播を用いれば、高速に効率良く勾配を求めることができる。\n",
    "\n",
    "\n",
    "\n",
    "### 5.7.2 誤差逆伝播に対応したニューラルネットワークの実装\n",
    "ここでは、2層のニューラルネットを実装する。また、前章からの主な変更点は、<b>レイヤ</b>を使用していることである。レイヤを使用することで、認識結果を得る処理や勾配を求める処理がレイヤの伝播だけで達成することができるようになる。\n",
    "<br>（この本では重みを持つ層を1層、2層と数えるようなので、以下のように2層ニューラルネットとなる）\n",
    "\n",
    "<br><br>\n",
    "<img src = \".\\img\\5-34実装版のニューラルネットのイメージ.png\" width=\"500\" height=\"400\">\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#勉強のため、ファイルではなく明示的にクラスを定義する\n",
    "########################各レイヤの中身を定義するところ########################\n",
    "import sys, os\n",
    "# 親ディレクトリのファイルをインポートするための設定\n",
    "sys.path.append(\"C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "#from common.functions import *\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "\n",
    "#Affineレイヤ定義　⇒Class No:１\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        # 重み（W）・バイアス（b）パラメータを格納する変数\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        #2次元整形した行列（テンソル対応したもの）を保存する変数と元々の行列の次元を保持する変数\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        \n",
    "        # 重み・バイアスパラメータを偏微分した結果を格納する変数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    #順伝播処理\n",
    "    def forward(self, x):\n",
    "        #元の次元数をバックアップ \n",
    "        self.original_x_shape = x.shape\n",
    "        #テンソル対応(多次元配列の次元を一つ減らす⇒今回は意味なし？)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        #整形した行列をクラス変数として保持\n",
    "        self.x = x\n",
    "        #Affineレイヤの順伝播は重み付き信号の総和を伝播する\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ##ｐ148～P151\n",
    "        #dout（前のレイヤからの逆伝播値）とwを転置したものの内積をとる=ｘ：入力の逆伝播値\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        #xを転置したものと、dout（前のレイヤからの逆伝播値）の内積をとる=重みの逆伝播値\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        #以前よくわからなかったところ。相変わらずよくわからん。bの逆伝播値：前レイヤを合計して行減らす\n",
    "        #print(\"バイアス逆伝播前：\",dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        #print(\"バイアス逆伝播値：\",self.db)\n",
    "        # 入力データの形状に戻す（テンソル対応）\n",
    "        dx = dx.reshape(*self.original_x_shape)  \n",
    "        return dx\n",
    "\n",
    "\n",
    "#Reluレイヤ定義　⇒Class No:2\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #与えられた値が0より大きければそのまま伝播（Relu）\n",
    "        self.mask = (x <= 0) #0以下のTrue/False判定\n",
    "        out = x.copy() #元データリターンする変数にコピー\n",
    "        out[self.mask] = 0 #Trueの箇所（0以下の箇所）を0に置き換え\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #逆伝播された値が0より大きければそのまま逆伝播（Relu）\n",
    "        dout[self.mask] = 0 #伝播された来た値のTrueの箇所（0以下の箇所）を0に置き換え\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "#Softmax&Loss（交差エントロピー誤差）レイヤ定義　⇒Class No:3\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        #教師データのベクトルの要素数を取得\n",
    "        batch_size = self.t.shape[0]\n",
    "        # 教師データがone-hot表現か判定（教師データ要素数=softmaxの出力数)P92\n",
    "        if self.t.size == self.y.size: \n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "########################各レイヤの中身を定義終わり########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################定義した各レイヤを使ってニューラルネットを定義########################\n",
    "import sys, os\n",
    "# 親ディレクトリのファイルをインポートするための設定\n",
    "sys.path.append(\"C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "#理解するためにlayersファイル（各種レイヤのクラス定義）は上で定義する\n",
    "#from common.layers import *　\n",
    "from common.gradient import numerical_gradient #数値微分の関数を読込\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    #インスタンス作成時に実行\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化（pramsというクラス変数を持つ）\n",
    "        self.params = {}\n",
    "        #第１層の重みの初期値として、784×50行列を生成（値は標準正規分布からのランダム数×学習率：0.01）\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) \n",
    "        #第１層のバイアスの初期値として、50行のベクトルを生成（値は0）\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        #第2層の重みの初期値として、50×10行列を生成（値は標準正規分布からのランダム数×学習率：0.01）\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        #第2層のバイアスの初期値として、10行のベクトルを生成（値は0）\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成（layersというクラス変数を持つ）\n",
    "        self.layers = OrderedDict() #名前付きでデータを保存できるコレクションを定義\n",
    "        #Affineクラスのインスタンスを生成　⇒Class No:１\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) \n",
    "        #Reluクラスのインスタンスを生成　⇒Class No:2\n",
    "        self.layers['Relu1'] = Relu() \n",
    "        #Affineクラスのインスタンスを生成　⇒Class No:１\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2']) \n",
    "        #SoftmaxWithLossクラスのインスタンスを生成　⇒Class No:3\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "     \n",
    "    #predictメソッドを定義\n",
    "    def predict(self, x):\n",
    "        #レイヤのコレクション分ループ\n",
    "        for layer in self.layers.values():\n",
    "            #各レイヤのfoward（順伝播）処理を実行\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "########################ニューラルネットの定義終わり########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4　誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################定義したニューラルネットを使って学習を行う########################\n",
    "import sys, os\n",
    "# 親ディレクトリのファイルをインポートするための設定\n",
    "sys.path.append(\"C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "np.set_printoptions(precision=3) #有効桁３桁で丸める\n",
    "\n",
    "# データの読み込み(normalize:入力値を0.0～1.0に正規化するか、one_hot_label：正解不正解ラベルを0・1で表現するか)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "#x_trainの画像を確認するコード　⇒　処理が中断するのでコメントアウト。下で画像を読込\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True , normalize=False)\n",
    "#img = x_train[0]\n",
    "#print(img.shape) \n",
    "#img = img.reshape(28,28)\n",
    "#print(img.shape)\n",
    "#img = Image.fromarray(np.uint8(img))\n",
    "#img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■x_train[0]の画像を確認\n",
    "<br><br>\n",
    "<img src = \".\\img\\x_train[0].png\" width=\"200\" height=\"100\">\n",
    "<br><br>\n",
    "■x_train[0]のデータを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====x_train[0]=====\n",
      "x_train[0]の大きさ確認： (784,)\n",
      "[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.012\n",
      "  0.071  0.071  0.071  0.494  0.533  0.686  0.102  0.651  1.     0.969\n",
      "  0.498  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.118  0.141  0.369  0.604  0.667  0.992  0.992  0.992\n",
      "  0.992  0.992  0.882  0.675  0.992  0.949  0.765  0.251  0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.192  0.933\n",
      "  0.992  0.992  0.992  0.992  0.992  0.992  0.992  0.992  0.984  0.365\n",
      "  0.322  0.322  0.22   0.153  0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.071  0.859  0.992  0.992  0.992\n",
      "  0.992  0.992  0.776  0.714  0.969  0.945  0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.314  0.612  0.42   0.992  0.992  0.804  0.043  0.     0.169\n",
      "  0.604  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.055\n",
      "  0.004  0.604  0.992  0.353  0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.545  0.992  0.745\n",
      "  0.008  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.043  0.745  0.992  0.275  0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.137\n",
      "  0.945  0.882  0.627  0.424  0.004  0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.318  0.941  0.992  0.992\n",
      "  0.467  0.098  0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.176  0.729  0.992  0.992  0.588  0.106  0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.063\n",
      "  0.365  0.988  0.992  0.733  0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.976  0.992  0.976\n",
      "  0.251  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.18   0.51   0.718  0.992  0.992  0.812  0.008  0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.153  0.58   0.898  0.992  0.992  0.992\n",
      "  0.98   0.714  0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.094\n",
      "  0.447  0.867  0.992  0.992  0.992  0.992  0.788  0.306  0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.09   0.259  0.835  0.992  0.992  0.992  0.992\n",
      "  0.776  0.318  0.008  0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.071  0.671\n",
      "  0.859  0.992  0.992  0.992  0.992  0.765  0.314  0.035  0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.216  0.675  0.886  0.992  0.992  0.992  0.992  0.957\n",
      "  0.522  0.043  0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.533\n",
      "  0.992  0.992  0.992  0.831  0.529  0.518  0.063  0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      "=====t_train[0]=====\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "=====x_train=====\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"=====x_train[0]=====\")\n",
    "print(\"x_train[0]の大きさ確認：\",x_train[0].shape) #x_train[0]の行列の大きさを確認　⇒　1行784列\n",
    "print(x_train[0]) #どんな形式で格納されているか確認 normalize=trueしているので0～1のハズ\n",
    "print(\"=====t_train[0]=====\")\n",
    "print(t_train[0]) #正解ラベルが本当に５か確認\n",
    "print(\"=====x_train=====\")\n",
    "print(x_train.shape) #最後にx_trainにこんなデータがどのくらいあるのかチェック⇒60,000行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自作した2層レイヤのクラスをインスタンス化\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<img src = \".\\img\\5-33実装版のニューラルネットのイメージ.png\" width=\"500\" height=\"400\">\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600.0\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000 #イテレーション回数　（≒epoch数）\n",
    "train_size = x_train.shape[0] # トレーニングデータのデータ数（行数）\n",
    "batch_size = 100 # ミニバッチのサイズ\n",
    "learning_rate = 0.1 #学習率（1回の学習でどれだけパラメータを更新するか）の初期設定\n",
    "\n",
    "train_loss_list = [] #誤差の結果を格納する箱を用意\n",
    "train_acc_list = [] #トレーニングデータのaccuracyを格納する箱を用意\n",
    "test_acc_list = [] #テストデータのaccuracyを格納する箱を用意\n",
    "\n",
    "#１エポック指定：60000÷100=600　これは１epoch分（学習データを一回総なめしたタイミング）\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "print(iter_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ミニバッチサイズ： (100,)\n",
      "ミニバッチとして取得した添え字： [46094 20150  1067 39921 37927 14266 50336 18299 13596 54912 44368   656\n",
      " 44041 34848 45436 13064  6924 39363 16211 58606 18796 12496 21325 38239\n",
      " 31486 29165 15913 52465 47456 36249  1767 24142 42633 22087 26247   667\n",
      " 11188   367 53958 37477  4626  4104 19874 11586 39809 32553 55033 21894\n",
      " 59593 48251 12840   206 35632 48563 21517 15941 33283 58012 35473 28745\n",
      " 52420  4506  6151 50569 37703 44979  9085  4175 18724  5583 41945 36465\n",
      " 21404   855 49927 25742 38958  2243 17615 47904 42877   785  2562 14861\n",
      " 10662 35221  6300 27982 37691 31480 46856 19519  9369 20182  2217 48104\n",
      " 57545 22192 44610 16632]\n",
      "[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.012\n",
      "  0.149  0.486  0.973  1.     0.835  0.333  0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.188  0.447  0.737  0.992  0.992  0.992  0.992\n",
      "  0.992  0.965  0.231  0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.153  0.827\n",
      "  0.945  0.992  0.992  0.992  0.992  0.973  0.976  0.992  0.992  0.318  0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.004  0.427  0.894  0.992  0.992  0.992  0.984  0.733  0.4\n",
      "  0.027  0.49   0.992  0.992  0.318  0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.161  0.992\n",
      "  0.992  0.992  0.796  0.353  0.235  0.     0.     0.067  0.91   0.992\n",
      "  0.914  0.082  0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.447  0.992  0.643  0.149  0.039  0.\n",
      "  0.     0.     0.     0.306  0.992  0.992  0.506  0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.18   0.549  0.098  0.     0.     0.     0.     0.     0.071  0.945\n",
      "  0.992  0.992  0.384  0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.506  0.992  0.992  0.553  0.012  0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.408\n",
      "  0.976  0.992  0.98   0.176  0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.016  0.714  0.992  0.976  0.42   0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.22\n",
      "  0.992  0.992  0.753  0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.212  0.929  0.992  0.808  0.055  0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.227\n",
      "  0.918  0.992  0.945  0.306  0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.2    0.922  0.992  0.992  0.439  0.     0.\n",
      "  0.09   0.075  0.208  0.384  0.231  0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.173\n",
      "  0.929  0.992  0.992  0.482  0.039  0.263  0.749  0.965  0.8    0.776\n",
      "  0.992  0.976  0.02   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.608  0.992  0.992  0.992\n",
      "  0.522  0.745  0.992  0.992  0.992  0.992  0.859  0.737  0.847  0.016  0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.071  0.843  0.992  0.992  0.992  0.992  0.992  0.992  0.992\n",
      "  0.725  0.431  0.078  0.     0.075  0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.004  0.639  0.992\n",
      "  0.992  0.992  0.992  0.992  0.992  0.737  0.227  0.008  0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.016  0.882  0.992  0.992  0.992  0.898  0.51\n",
      "  0.294  0.063  0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.341  0.749  0.569  0.31   0.098  0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.     0.     0.     0.     0.   ]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#トレーニングデータ(6万件)からランダムにミニバッチ分（100件）データを取得する\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(\"ミニバッチサイズ：\",batch_mask.shape)\n",
    "print(\"ミニバッチとして取得した添え字：\",batch_mask)\n",
    "print(x_batch[0])\n",
    "print(t_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.128383333333 0.136\n",
      "600 0.904283333333 0.9065\n",
      "1200 0.926333333333 0.93\n",
      "1800 0.93815 0.9384\n",
      "2400 0.942533333333 0.9408\n",
      "3000 0.950716666667 0.9463\n",
      "3600 0.956916666667 0.9531\n",
      "4200 0.961416666667 0.9569\n",
      "4800 0.964016666667 0.9573\n",
      "5400 0.966783333333 0.9596\n",
      "6000 0.97 0.9635\n",
      "6600 0.97215 0.9648\n",
      "7200 0.973116666667 0.9666\n",
      "7800 0.97465 0.9658\n",
      "8400 0.976266666667 0.9678\n",
      "9000 0.978266666667 0.9695\n",
      "9600 0.977966666667 0.9691\n"
     ]
    }
   ],
   "source": [
    "#イテレーション回数分ループ（epoch換算では、10000÷600=16～17epoch）\n",
    "for i in range(iters_num):\n",
    "    #トレーニングデータ(6万件)からランダムにミニバッチ分（100件）データを取得する\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #勾配を計算（TwoLayerNetクラスのgradientメソッド呼び出し）\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    #重みとバイアスを更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    #交差エントロピー誤差を計算（TwoLayerNetクラスのlossメソッド呼び出し）\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    #誤差を格納する箱に結果を入れる\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    #ループ回数が600の倍数の時（１epoch終了毎）、途中経過を表示する\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(i, train_acc, test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3　誤差逆伝播法の勾配確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.08622340559e-13\n",
      "b1:1.36124966551e-12\n",
      "W2:1.11549492218e-12\n",
      "b2:1.19237948681e-10\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "# 親ディレクトリのファイルをインポートするための設定\n",
    "sys.path.append(\"C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8　まとめ\n",
    "第5章では、視覚的に計算の過程を表す計算グラフという方法を学んだ。この計算グラフを用いて誤差逆伝播法を学び、また、ニューラルネットワークで行う処理をレイヤという単位で実装した（Affineレイヤ、Softmaxレイヤなど）。各レイヤはforwardとbackwardというメソッドが実装されており、データを順伝播、逆伝播することで重みパラメータの勾配を効率的に求めることができる。\n",
    "<br>このレイヤを自由に組み合わせることで、自分の好きなネットワークを<b>簡単に？</b>作ることができる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
