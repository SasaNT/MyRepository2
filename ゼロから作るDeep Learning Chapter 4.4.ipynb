{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4　勾配\n",
    "\n",
    "　前回は、x<sub>0</sub>とx<sub>1</sub>の偏微分係数を個別に計算する関数を作成しました。今回は、まずx<sub>0</sub>とx<sub>1</sub>の偏微分係数をまとめて計算する関数を作成します。例えば、x<sub>0</sub>=3、x<sub>1</sub>=4の時の(x<sub>0</sub>,x<sub>1</sub>)の両方の偏微分係数を$\\begin{pmatrix}\\frac{ \\partial f }{ \\partial x } & \\frac{ \\partial f }{ \\partial x }\\end{pmatrix}$として計算することを考えます。\n",
    "<br>なお、すべての変数の偏微分係数をベクトルとしてまとめたものを<font size=\"4\" color=\"red\">勾配(gradient)ベクトル</font>と言います。では実装してみましょう。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#ｆ：関数、ｘ：Numpy配列の引数をとる関数\n",
    "def numerical_gradient(f,x):\n",
    "    h=1e-4 #0.0001\n",
    "    \n",
    "    #xと同じ形状で全部ゼロの配列を作成\n",
    "    grad = np.zeros_like(x)\n",
    "    print(\"np.zeros_like(x)：\")\n",
    "    print(np.zeros_like(x))\n",
    "    print(\"x.size：\")\n",
    "    print(x.size)\n",
    "    \n",
    "    #Nmpy配列xのサイズ（要素）分ループ\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h)の計算\n",
    "        x[idx] = tmp_val + h\n",
    "        print(\"x+h：\")\n",
    "        print(x)\n",
    "        fxh1 = f(x)\n",
    "        print(\"f(x+h)：\")\n",
    "        print(fxh1)\n",
    "        \n",
    "        #f(x-h)の計算\n",
    "        x[idx] = tmp_val - h\n",
    "        print(\"x-h：\")\n",
    "        print(x)\n",
    "        fxh2 = f(x)\n",
    "        print(\"f(x-h)：\")\n",
    "        print(fxh2)\n",
    "        \n",
    "        #中心差分による数値微分\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        print(\"中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\")\n",
    "        print((fxh1 - fxh2) / (2*h))\n",
    "        \n",
    "        #値をもとに戻す\n",
    "        x[idx] = tmp_val \n",
    "\n",
    "    print(\"勾配：\")\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "それでは、点(3,4),(0,2),(3,0)での勾配を求めてみましょう。\n",
    "その前に、関数（モデル）\\begin{eqnarray} f(x_0,x_1) = x_0^2 + x_1^2\\end{eqnarray}を再定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_2([3.0,4.0]])：\n",
      "25.0\n",
      "np.zeros_like(x)：\n",
      "[ 0.  0.]\n",
      "x.size：\n",
      "2\n",
      "x+h：\n",
      "[ 3.0001  4.    ]\n",
      "f(x+h)：\n",
      "25.00060001\n",
      "x-h：\n",
      "[ 2.9999  4.    ]\n",
      "f(x-h)：\n",
      "24.99940001\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "6.0\n",
      "x+h：\n",
      "[ 3.      4.0001]\n",
      "f(x+h)：\n",
      "25.00080001\n",
      "x-h：\n",
      "[ 3.      3.9999]\n",
      "f(x-h)：\n",
      "24.99920001\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "8.0\n",
      "勾配：\n",
      "[ 6.  8.]\n"
     ]
    }
   ],
   "source": [
    "#関数再定義\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "print(\"function_2([3.0,4.0]])：\")\n",
    "print(function_2(np.array([3.0,4.0])))\n",
    "#print(function_2(np.array([0.0,2.0])))\n",
    "#print(function_2(np.array([3.0,0.0])))\n",
    "\n",
    "#勾配の計算\n",
    "print(numerical_gradient(function_2,np.array([3.0,4.0])))\n",
    "#print(numerical_gradient(function_2,np.array([0.0,2.0])))\n",
    "#print(numerical_gradient(function_2,np.array([3.0,0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "※実際は、[6.000000000037801,799999999991189]という値が得られますが、[6. , 8.]として出力されます。これは、Numpy配列を出力するときには、数値が『見やすいように』整形して出力されるためです。\n",
    "<br>\n",
    "この例では、点(3,4)の勾配は(6,8)という結果になりましたが、この勾配は何を意味しているのでしょうか？それを理解するため\\begin{eqnarray} f(x_0,x_1) = x_0^2 + x_1^2\\end{eqnarray}の勾配を図で表してみることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros_like(x)：\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "x.size：\n",
      "648\n",
      "x+h：\n",
      "[[ -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00\n",
      "   -1.99990000e+00  -1.74990000e+00  -1.49990000e+00  -1.24990000e+00\n",
      "   -9.99900000e-01  -7.49900000e-01  -4.99900000e-01  -2.49900000e-01\n",
      "    1.00000000e-04   2.50100000e-01   5.00100000e-01   7.50100000e-01\n",
      "    1.00010000e+00   1.25010000e+00   1.50010000e+00   1.75010000e+00\n",
      "    2.00010000e+00   2.25010000e+00  -1.99990000e+00  -1.74990000e+00\n",
      "   -1.49990000e+00  -1.24990000e+00  -9.99900000e-01  -7.49900000e-01\n",
      "   -4.99900000e-01  -2.49900000e-01   1.00000000e-04   2.50100000e-01\n",
      "    5.00100000e-01   7.50100000e-01   1.00010000e+00   1.25010000e+00\n",
      "    1.50010000e+00   1.75010000e+00   2.00010000e+00   2.25010000e+00]\n",
      " [ -2.00000000e+00  -2.00000000e+00  -2.00000000e+00  -2.00000000e+00\n",
      "   -2.00000000e+00  -2.00000000e+00  -2.00000000e+00  -2.00000000e+00\n",
      "   -2.00000000e+00  -2.00000000e+00  -2.00000000e+00  -2.00000000e+00\n",
      "   -2.00000000e+00  -2.00000000e+00  -2.00000000e+00  -2.00000000e+00\n",
      "   -2.00000000e+00  -2.00000000e+00  -1.75000000e+00  -1.75000000e+00\n",
      "   -1.75000000e+00  -1.75000000e+00  -1.75000000e+00  -1.75000000e+00\n",
      "   -1.75000000e+00  -1.75000000e+00  -1.75000000e+00  -1.75000000e+00\n",
      "   -1.75000000e+00  -1.75000000e+00  -1.75000000e+00  -1.75000000e+00\n",
      "   -1.75000000e+00  -1.75000000e+00  -1.75000000e+00  -1.75000000e+00\n",
      "   -1.50000000e+00  -1.50000000e+00  -1.50000000e+00  -1.50000000e+00\n",
      "   -1.50000000e+00  -1.50000000e+00  -1.50000000e+00  -1.50000000e+00\n",
      "   -1.50000000e+00  -1.50000000e+00  -1.50000000e+00  -1.50000000e+00\n",
      "   -1.50000000e+00  -1.50000000e+00  -1.50000000e+00  -1.50000000e+00\n",
      "   -1.50000000e+00  -1.50000000e+00  -1.25000000e+00  -1.25000000e+00\n",
      "   -1.25000000e+00  -1.25000000e+00  -1.25000000e+00  -1.25000000e+00\n",
      "   -1.25000000e+00  -1.25000000e+00  -1.25000000e+00  -1.25000000e+00\n",
      "   -1.25000000e+00  -1.25000000e+00  -1.25000000e+00  -1.25000000e+00\n",
      "   -1.25000000e+00  -1.25000000e+00  -1.25000000e+00  -1.25000000e+00\n",
      "   -1.00000000e+00  -1.00000000e+00  -1.00000000e+00  -1.00000000e+00\n",
      "   -1.00000000e+00  -1.00000000e+00  -1.00000000e+00  -1.00000000e+00\n",
      "   -1.00000000e+00  -1.00000000e+00  -1.00000000e+00  -1.00000000e+00\n",
      "   -1.00000000e+00  -1.00000000e+00  -1.00000000e+00  -1.00000000e+00\n",
      "   -1.00000000e+00  -1.00000000e+00  -7.50000000e-01  -7.50000000e-01\n",
      "   -7.50000000e-01  -7.50000000e-01  -7.50000000e-01  -7.50000000e-01\n",
      "   -7.50000000e-01  -7.50000000e-01  -7.50000000e-01  -7.50000000e-01\n",
      "   -7.50000000e-01  -7.50000000e-01  -7.50000000e-01  -7.50000000e-01\n",
      "   -7.50000000e-01  -7.50000000e-01  -7.50000000e-01  -7.50000000e-01\n",
      "   -5.00000000e-01  -5.00000000e-01  -5.00000000e-01  -5.00000000e-01\n",
      "   -5.00000000e-01  -5.00000000e-01  -5.00000000e-01  -5.00000000e-01\n",
      "   -5.00000000e-01  -5.00000000e-01  -5.00000000e-01  -5.00000000e-01\n",
      "   -5.00000000e-01  -5.00000000e-01  -5.00000000e-01  -5.00000000e-01\n",
      "   -5.00000000e-01  -5.00000000e-01  -2.50000000e-01  -2.50000000e-01\n",
      "   -2.50000000e-01  -2.50000000e-01  -2.50000000e-01  -2.50000000e-01\n",
      "   -2.50000000e-01  -2.50000000e-01  -2.50000000e-01  -2.50000000e-01\n",
      "   -2.50000000e-01  -2.50000000e-01  -2.50000000e-01  -2.50000000e-01\n",
      "   -2.50000000e-01  -2.50000000e-01  -2.50000000e-01  -2.50000000e-01\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   2.50000000e-01   2.50000000e-01\n",
      "    2.50000000e-01   2.50000000e-01   2.50000000e-01   2.50000000e-01\n",
      "    2.50000000e-01   2.50000000e-01   2.50000000e-01   2.50000000e-01\n",
      "    2.50000000e-01   2.50000000e-01   2.50000000e-01   2.50000000e-01\n",
      "    2.50000000e-01   2.50000000e-01   2.50000000e-01   2.50000000e-01\n",
      "    5.00000000e-01   5.00000000e-01   5.00000000e-01   5.00000000e-01\n",
      "    5.00000000e-01   5.00000000e-01   5.00000000e-01   5.00000000e-01\n",
      "    5.00000000e-01   5.00000000e-01   5.00000000e-01   5.00000000e-01\n",
      "    5.00000000e-01   5.00000000e-01   5.00000000e-01   5.00000000e-01\n",
      "    5.00000000e-01   5.00000000e-01   7.50000000e-01   7.50000000e-01\n",
      "    7.50000000e-01   7.50000000e-01   7.50000000e-01   7.50000000e-01\n",
      "    7.50000000e-01   7.50000000e-01   7.50000000e-01   7.50000000e-01\n",
      "    7.50000000e-01   7.50000000e-01   7.50000000e-01   7.50000000e-01\n",
      "    7.50000000e-01   7.50000000e-01   7.50000000e-01   7.50000000e-01\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.25000000e+00   1.25000000e+00\n",
      "    1.25000000e+00   1.25000000e+00   1.25000000e+00   1.25000000e+00\n",
      "    1.25000000e+00   1.25000000e+00   1.25000000e+00   1.25000000e+00\n",
      "    1.25000000e+00   1.25000000e+00   1.25000000e+00   1.25000000e+00\n",
      "    1.25000000e+00   1.25000000e+00   1.25000000e+00   1.25000000e+00\n",
      "    1.50000000e+00   1.50000000e+00   1.50000000e+00   1.50000000e+00\n",
      "    1.50000000e+00   1.50000000e+00   1.50000000e+00   1.50000000e+00\n",
      "    1.50000000e+00   1.50000000e+00   1.50000000e+00   1.50000000e+00\n",
      "    1.50000000e+00   1.50000000e+00   1.50000000e+00   1.50000000e+00\n",
      "    1.50000000e+00   1.50000000e+00   1.75000000e+00   1.75000000e+00\n",
      "    1.75000000e+00   1.75000000e+00   1.75000000e+00   1.75000000e+00\n",
      "    1.75000000e+00   1.75000000e+00   1.75000000e+00   1.75000000e+00\n",
      "    1.75000000e+00   1.75000000e+00   1.75000000e+00   1.75000000e+00\n",
      "    1.75000000e+00   1.75000000e+00   1.75000000e+00   1.75000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "    2.00000000e+00   2.00000000e+00   2.25000000e+00   2.25000000e+00\n",
      "    2.25000000e+00   2.25000000e+00   2.25000000e+00   2.25000000e+00\n",
      "    2.25000000e+00   2.25000000e+00   2.25000000e+00   2.25000000e+00\n",
      "    2.25000000e+00   2.25000000e+00   2.25000000e+00   2.25000000e+00\n",
      "    2.25000000e+00   2.25000000e+00   2.25000000e+00   2.25000000e+00]]\n",
      "f(x+h)：\n",
      "[ 550.13310324  550.125     ]\n",
      "x-h：\n",
      "[[-2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25\n",
      "  -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "   1.    1.25  1.5   1.75  2.    2.25 -2.   -1.75 -1.5  -1.25 -1.   -0.75\n",
      "  -0.5  -0.25  0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25]\n",
      " [-2.   -2.   -2.   -2.   -2.   -2.   -2.   -2.   -2.   -2.   -2.   -2.   -2.\n",
      "  -2.   -2.   -2.   -2.   -2.   -1.75 -1.75 -1.75 -1.75 -1.75 -1.75 -1.75\n",
      "  -1.75 -1.75 -1.75 -1.75 -1.75 -1.75 -1.75 -1.75 -1.75 -1.75 -1.75 -1.5\n",
      "  -1.5  -1.5  -1.5  -1.5  -1.5  -1.5  -1.5  -1.5  -1.5  -1.5  -1.5  -1.5\n",
      "  -1.5  -1.5  -1.5  -1.5  -1.5  -1.25 -1.25 -1.25 -1.25 -1.25 -1.25 -1.25\n",
      "  -1.25 -1.25 -1.25 -1.25 -1.25 -1.25 -1.25 -1.25 -1.25 -1.25 -1.25 -1.   -1.\n",
      "  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  -1.   -1.   -1.   -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75\n",
      "  -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.5  -0.5  -0.5\n",
      "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
      "  -0.5  -0.5  -0.5  -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25\n",
      "  -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25  0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n",
      "   0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.5   0.5   0.5   0.5   0.5\n",
      "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5\n",
      "   0.5   0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75\n",
      "   0.75  0.75  0.75  0.75  0.75  0.75  0.75  1.    1.    1.    1.    1.    1.\n",
      "   1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1.\n",
      "   1.25  1.25  1.25  1.25  1.25  1.25  1.25  1.25  1.25  1.25  1.25  1.25\n",
      "   1.25  1.25  1.25  1.25  1.25  1.25  1.5   1.5   1.5   1.5   1.5   1.5\n",
      "   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5   1.5\n",
      "   1.75  1.75  1.75  1.75  1.75  1.75  1.75  1.75  1.75  1.75  1.75  1.75\n",
      "   1.75  1.75  1.75  1.75  1.75  1.75  2.    2.    2.    2.    2.    2.    2.\n",
      "   2.    2.    2.    2.    2.    2.    2.    2.    2.    2.    2.    2.25\n",
      "   2.25  2.25  2.25  2.25  2.25  2.25  2.25  2.25  2.25  2.25  2.25  2.25\n",
      "   2.25  2.25  2.25  2.25  2.25]]\n",
      "f(x-h)：\n",
      "[ 550.125  550.125]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2) into shape (324)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3aa05ddc5ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6ea1ddd937eb>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[1;31m#中心差分による数値微分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2) into shape (324)"
     ]
    }
   ],
   "source": [
    "###jupyterとゼロディープで使うモジュールのある場所が異なるのでこのように章のディレクトリパスをはる\n",
    "###パスは皆さんの環境に合せて書き換えてください\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\zero_DL\\\\ch04')\n",
    "\n",
    "###メイン処理開始\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def function_3(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "\n",
    "##-2から2.5まで0.25刻みのデータを生成\n",
    "x0 = np.arange(-2, 2.5, 0.25)\n",
    "x1 = np.arange(-2, 2.5, 0.25)\n",
    "#print(x0)\n",
    "\n",
    "##配列の要素から格子列を生成する\n",
    "X, Y = np.meshgrid(x0, x1)\n",
    "#print(X,Y)\n",
    "\n",
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "\n",
    "grad = numerical_gradient(function_3, np.array([X, Y]) )\n",
    "\n",
    "plt.figure()\n",
    "plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "plt.xlim([-2, 2.5])\n",
    "plt.ylim([-2, 2.5])\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この図のように、勾配は関数$\\begin{eqnarray} f(x_0,x_1) = x_0^2 + x_1^2\\end{eqnarray}$の、一番低い場所を指す向きを持ったベクトルとして図示されます。正確に言うと、勾配が示す方向は、各場所（点）において関数の値を最も減らす方向を示します。これは重要なポイントです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4.4.1 勾配法\n",
    "　ニューラルネットワークも他の機械学習と同様に、学習の際に最適なパラメータ（重みとバイアス）を探索します。本書では、最適なパラメータを勾配を利用して損失関数関数の最小値を求めることで決定します（勾配法）。<br>\n",
    "　ここでの注意点は、各地点において関数の値を最も減らす方向を示すのが勾配ということです。そのため、勾配が指す先が本当に関数の最小値なのか、またその方向が本当に進むべき方向なのかどうか保証することはできません。<br>\n",
    " 関数の最小値、極小値<sub>※１</sub>、鞍点（saddle point）<sub>※2</sub>と呼ばれる場所では、勾配が0となりますので、必ずしも勾配=０が最小値であるとは限りません。<br>\n",
    "　　※1：極小値とは、局所的な最小値。つまりある点に限定した場合に最小値となる点です。<br>\n",
    "　　※2：鞍点とは、ある方向で見れば極大値で、ある方向で見れば極小値となる点です。<br>\n",
    "　実際複雑な関数では、平らな土地に入り込み「プラトー」と呼ばれる学習が進まない停滞期に陥ってしまうことが多く、勾配の指す方向が最小値でない場合がほとんどです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 鞍点の話書きたい。プラトーとか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　勾配の方向が最小値であるとは限らないにせよ、その方向に進むことで関数の値を最も減らすことができるため、勾配の情報を手掛かりに探索を行うべきでしょう。<br>\n",
    "　勾配法では、現在の場所から勾配方向に一定の距離だけ進み、進んだ先でも同様に勾配を求めて同様に移動します。この作業を繰り返えすことで関数の値を徐々に減らしていきます。それでは勾配法を数式で表してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{eqnarray}x_0 = x_0 - \\eta\\frac{ \\partial f }{ \\partial x_0 }\\end{eqnarray}<br>\n",
    "\\begin{eqnarray}x_1 = x_1 - \\eta\\frac{ \\partial f }{ \\partial x_1 }\\end{eqnarray}<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "　$\\eta$(イータ)はニューラルネットの１回の学習におけるパラメータの更新量を表し学習率（learning tate）と呼ばれます。ニューラルネットではこのステップを繰り返すことで変数の値を更新していき関数の値を徐々に減らしていきます。<br>\n",
    "　なお、学習率の値は0.01や0.001など前もって何らかの値を設定する必要があり、一般にこの値は大きすぎても小さすぎても、「良い場所」にたどり着くことができないと言われています。それでは、勾配降下法を実装してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        #１から10ステップまでのXの減少具合を表示\n",
    "        if i < 5:\n",
    "            print(str(i) + \"回目のx\")\n",
    "            print(x)\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        \n",
    "        #今回の勾配を表示\n",
    "        if i < 5:\n",
    "            print(str(i) + \"回目のgrad\")\n",
    "            print(grad)\n",
    "        \n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "#先ほどのprint文がじゃまなので消したものを再定義\n",
    "def numerical_gradient(f,x):\n",
    "    h=1e-4 #0.0001\n",
    "    \n",
    "    #xと同じ形状で全部ゼロの配列を作成\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    #Nmpy配列xのサイズ（要素）分ループ\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h)の計算\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h)の計算\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "       \n",
    "        #中心差分による数値微分\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        #値をもとに戻す\n",
    "        x[idx] = tmp_val \n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 引数のfは最適化したい関数、init_xは初期値、lrはlearning rate（学習率）、step_numは勾配法による繰り返し回数です。関数の勾配をnumerical_gradient(f, x)で求め、その値に学習率を乗じた値で更新します。この関数を使えば関数の極小値を求めることができ、うまくいけば最小値を求めることができます。<br>\n",
    " それでは、$\\begin{eqnarray} f(x_0,x_1) = x_0^2 + x_1^2\\end{eqnarray}$の最小値（または極小値）を勾配法で求めてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0回目のx\n",
      "[-3.  4.]\n",
      "0回目のgrad\n",
      "[-6.  8.]\n",
      "1回目のx\n",
      "[-2.4  3.2]\n",
      "1回目のgrad\n",
      "[-4.8  6.4]\n",
      "2回目のx\n",
      "[-1.92  2.56]\n",
      "2回目のgrad\n",
      "[-3.84  5.12]\n",
      "3回目のx\n",
      "[-1.536  2.048]\n",
      "3回目のgrad\n",
      "[-3.072  4.096]\n",
      "4回目のx\n",
      "[-1.2288  1.6384]\n",
      "4回目のgrad\n",
      "[-2.4576  3.2768]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def function_2(x):\n",
    "#    return x[0]**2 + x[1]**2\n",
    "#\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　この例では、初期値を（-3.0,4.0）として、勾配法を使った最小値探索を行っています。最終結果は(-6.1e-10, 8.1e-10)となり、\n",
    "真の最小値(0, 0)にほぼ近い結果を得ることができました。<br>\n",
    "　なお、勾配法のプロセスを図示すると、以下の通り原点（最小値）に徐々に近づいていることが分かります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAFyCAYAAACtP0M/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xae390e74a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###jupyterとゼロディープで使うモジュールのある場所が異なるのでこのように章のディレクトリパスをはる\n",
    "###パスは皆さんの環境に合せて書き換えてください\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\zero_DL\\\\ch04')\n",
    "\n",
    "###メイン処理開始\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient\n",
    "\n",
    "def gradient_descent_2(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent_2(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ここで、学習率が大きすぎる場合と小さすぎる場合に勾配法はどのような結果になるか実験してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.58983747e+13,  -1.29524862e+12])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##学習率が大きすぎる場合\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 10.0, step_num = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##学習率が小さすぎる場合\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " このように、学習率が大きすぎると大きな値に発散してしまいますし、逆に学習率が小さすぎると、ほとんど更新されずに終わってしまいます。\n",
    "つまり、適切な学習率を設定することが重要な問題となります。<br>\n",
    "※学習率のようなパラメータは<b>ハイパーパラメータ</b>と言います。これはニューラルネットのパラメータ（重みやバイアス）とは性質の異なるパラメータです。ニューラルネットのパラメータは学習によって自動で獲得されるのに対し、ハイパーパラメータは人の手で設定する必要があります。<br><br>\n",
    "\n",
    "# 4.4.2 ニューラルネットワークに対する勾配\n",
    "\n",
    "　ニューラルネットワークに対する勾配とは、重みパラメータに関する損失関数の勾配です。例えば、2×3の重み$\\boldsymbol{ W }$の損失関数$\\boldsymbol{ L }$の勾配は\n",
    " <br>\n",
    " <br>\n",
    "  \\begin{eqnarray} \\boldsymbol{ W } = \\left(\\begin{array}{cccc}\n",
    "    w_{ 11 } & w_{ 21 } & w_{ 31 } \\\\\n",
    "    w_{ 12 } & w_{ 22 } & w_{ 32 } \\\\\n",
    "  \\end{array}\\right)\\end{eqnarray}<br>\n",
    "  \\begin{eqnarray} \\frac{ \\partial L }{ \\partial \\boldsymbol{ W } } = \\left(\\begin{array}{cccc}\n",
    "    \\frac{ \\partial L }{ \\partial \\boldsymbol{ w_{ 11 }  }} & \\frac{ \\partial L }{ \\partial \\boldsymbol{ w_{ 21 }  }} & \\frac{ \\partial L }{ \\partial \\boldsymbol{ w_{ 31 }  }} \\\\\n",
    "    \\frac{ \\partial L }{ \\partial \\boldsymbol{ w_{ 12 }  }} & \\frac{ \\partial L }{ \\partial \\boldsymbol{ w_{ 22 }  }} & \\frac{ \\partial L }{ \\partial \\boldsymbol{ w_{ 32 }  }} \\\\\n",
    "  \\end{array}\\right)\\end{eqnarray}\n",
    "  <br>\n",
    "となります。<br>\n",
    "　$\\frac{ \\partial L }{ \\partial \\boldsymbol{ W }}$の各要素はそれぞの要素に関する偏微分から構成されます。例えば、1行目1列の要素である$ \\frac{ \\partial L }{ \\partial \\boldsymbol{ w_{ 11 }  }}$はw<sub>11</sub>を少し変化させると損失関数$\\boldsymbol{ L }$がどれだけ変化するかということを示しています。ここでは$\\frac{ \\partial L }{ \\partial \\boldsymbol{ W }}$の形状が、$\\boldsymbol{ W }$と同じであるということです。<br>\n",
    "　それでは、簡単なニューラルネットを例に実際に勾配を求める方法を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###jupyterとゼロディープで使うモジュールのある場所が異なるのでこのように章のディレクトリパスをはる\n",
    "###パスは皆さんの環境に合せて書き換えてください\n",
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\satoshi\\\\Desktop\\\\DeepLearning_Study\\\\zero_DL\\\\')\n",
    "\n",
    "##メイン処理（クラス宣言）\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        np.random.seed(20)\n",
    "        #正規(ガウス)分布で初期化\n",
    "        self.W = np.random.randn(2,3)\n",
    "        #print(\"重みパラメータ（self.W）:\")\n",
    "        #print(self.W)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "　simpleNetというクラスですが、これは2×3の重みパラメータをひとつだけインスタンス変数として持ちます。また、予測するためのpredict(x)と\n",
    "損失関数の値を求めるためのloss(x, t)の2つのメソッドがあります。lossメソッドのxには入力データ、tには正解ラベルが入力されます。<br>\n",
    "　それでは、simpleNetを使ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重みパラメータ（net.W）:\n",
      "[[ 0.88389311  0.19586502  0.35753652]\n",
      " [-2.34326191 -1.08483259  0.55969629]]\n",
      "p:\n",
      "[-1.57859985 -0.85883032  0.71824857]\n",
      "softmax関数をかます：\n",
      "[ 0.07694227  0.15803633  0.7650214 ]\n",
      "最大インデックス：\n",
      "2\n",
      "net.loss(x, t):\n",
      "0.267851467497\n",
      "-log0.7650214：\n",
      "0.267851471691\n"
     ]
    }
   ],
   "source": [
    "#インスタンスの生成\n",
    "net = simpleNet()\n",
    "#print(\"netインスタンス:\")\n",
    "#print(net)\n",
    "\n",
    "print(\"重みパラメータ（net.W）:\")\n",
    "print(net.W)#重みパラメータ（Self.Wと同値）\n",
    "\n",
    "##これはスタート地点だから適当な値？？\n",
    "x = np.array([0.6, 0.9])\n",
    "\n",
    "#予測？⇒ｘとself.W(net.W)の内積（1×2と2×3の内積）\n",
    "p = net.predict(x)\n",
    "print(\"p:\")\n",
    "print(p)\n",
    "\n",
    "##loss関数の中でやっていること：予測値ｐをsoftmax関数に入れている→出力を確率表現に\n",
    "y = softmax(p)\n",
    "print(\"softmax関数をかます：\")\n",
    "print(y)\n",
    "\n",
    "np.argmax(p)#最大値のインデックス\n",
    "print(\"最大インデックス：\")\n",
    "print(np.argmax(p))\n",
    "\n",
    "t = np.array([0, 0, 1])#正解ラベル\n",
    "\n",
    "##損失関数（交差エントロピー誤差）：今回正解（インデックス２）の出力は0.7650214なので\n",
    "##損失関数の結果は -log0.7650214　となるはず。。\n",
    "net.loss(x, t)\n",
    "print(\"net.loss(x, t):\")\n",
    "print(net.loss(x, t))\n",
    "print(\"-log0.7650214：\")\n",
    "print(-np.log(0.7650214))\n",
    "\n",
    "#f = lambda w: net.loss(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 続いて、勾配を求めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#勾配計算でやっていることの中身を見たいので。関数をそのままコピペしてpurint文を追加\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    ##x（net.W）にインデックスを振る\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    #ループ回数を表示する変数を定義\n",
    "    i = 0 \n",
    "    \n",
    "    ##インデックス分（重み行列の各要素について以下を実行）\n",
    "    while not it.finished:\n",
    "        i = i + 1 \n",
    "        print(\"＊＊＊＊＊＊＊\" + str(i) + \"回目のループ実行＊＊＊＊＊＊＊\")\n",
    "        \n",
    "        ##該当インデックス番号を取り出し、その値をtmp_valに入れる\n",
    "        idx = it.multi_index\n",
    "        print(\"idx\")\n",
    "        print(idx)\n",
    "        tmp_val = x[idx]\n",
    "        print(\"tmp_val\")\n",
    "        print(tmp_val)\n",
    " \n",
    "        ##その値をプラス方向に微小変化（ｈ）させた、重み行列を新たに用意して損失関数を計算\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        print(\"x+h\")\n",
    "        print(x)\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        print(\"net.loss(x+h,t)の結果：\")\n",
    "        print(fxh1)\n",
    "               \n",
    "        ##その値をマイナス方向に微小変化（ｈ）させた、重み行列を新たに用意して損失関数を計算\n",
    "        x[idx] = tmp_val - h \n",
    "        print(\"x-h\")\n",
    "        print(x)\n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        print(\"net.loss(x-h,t)の結果：\")\n",
    "        print(fxh2)\n",
    "\n",
    "        ##中心差分を取得する\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        print(\"中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\")\n",
    "        print((fxh1 - fxh2) / (2*h))\n",
    "\n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＊＊＊＊＊＊＊1回目のループ実行＊＊＊＊＊＊＊\n",
      "idx\n",
      "(0, 0)\n",
      "tmp_val\n",
      "0.883893112617\n",
      "x+h\n",
      "[[ 0.88399311  0.19586502  0.35753652]\n",
      " [-2.34326191 -1.08483259  0.55969629]]\n",
      "net.loss(x+h,t)の結果：\n",
      "0.267856084161\n",
      "x-h\n",
      "[[ 0.88379311  0.19586502  0.35753652]\n",
      " [-2.34326191 -1.08483259  0.55969629]]\n",
      "net.loss(x-h,t)の結果：\n",
      "0.267846851089\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "0.0461653617245\n",
      "＊＊＊＊＊＊＊2回目のループ実行＊＊＊＊＊＊＊\n",
      "idx\n",
      "(0, 1)\n",
      "tmp_val\n",
      "0.195865022038\n",
      "x+h\n",
      "[[ 0.88389311  0.19596502  0.35753652]\n",
      " [-2.34326191 -1.08483259  0.55969629]]\n",
      "net.loss(x+h,t)の結果：\n",
      "0.267860949916\n",
      "x-h\n",
      "[[ 0.88389311  0.19576502  0.35753652]\n",
      " [-2.34326191 -1.08483259  0.55969629]]\n",
      "net.loss(x-h,t)の結果：\n",
      "0.267841985557\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "0.0948217964034\n",
      "＊＊＊＊＊＊＊3回目のループ実行＊＊＊＊＊＊＊\n",
      "idx\n",
      "(0, 2)\n",
      "tmp_val\n",
      "0.357536515885\n",
      "x+h\n",
      "[[ 0.88389311  0.19586502  0.35763652]\n",
      " [-2.34326191 -1.08483259  0.55969629]]\n",
      "net.loss(x+h,t)の結果：\n",
      "0.267837369105\n",
      "x-h\n",
      "[[ 0.88389311  0.19586502  0.35743652]\n",
      " [-2.34326191 -1.08483259  0.55969629]]\n",
      "net.loss(x-h,t)の結果：\n",
      "0.267865566537\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "-0.14098715811\n",
      "＊＊＊＊＊＊＊4回目のループ実行＊＊＊＊＊＊＊\n",
      "idx\n",
      "(1, 0)\n",
      "tmp_val\n",
      "-2.34326190564\n",
      "x+h\n",
      "[[ 0.88389311  0.19586502  0.35753652]\n",
      " [-2.34316191 -1.08483259  0.55969629]]\n",
      "net.loss(x+h,t)の結果：\n",
      "0.267858392589\n",
      "x-h\n",
      "[[ 0.88389311  0.19586502  0.35753652]\n",
      " [-2.34336191 -1.08483259  0.55969629]]\n",
      "net.loss(x-h,t)の結果：\n",
      "0.267844542981\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "0.0692480426293\n",
      "＊＊＊＊＊＊＊5回目のループ実行＊＊＊＊＊＊＊\n",
      "idx\n",
      "(1, 1)\n",
      "tmp_val\n",
      "-1.08483258732\n",
      "x+h\n",
      "[[ 0.88389311  0.19586502  0.35753652]\n",
      " [-2.34326191 -1.08473259  0.55969629]]\n",
      "net.loss(x+h,t)の結果：\n",
      "0.267865691306\n",
      "x-h\n",
      "[[ 0.88389311  0.19586502  0.35753652]\n",
      " [-2.34326191 -1.08493259  0.55969629]]\n",
      "net.loss(x-h,t)の結果：\n",
      "0.267837244767\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "0.142232694667\n",
      "＊＊＊＊＊＊＊6回目のループ実行＊＊＊＊＊＊＊\n",
      "idx\n",
      "(1, 2)\n",
      "tmp_val\n",
      "0.559696289404\n",
      "x+h\n",
      "[[ 0.88389311  0.19586502  0.35753652]\n",
      " [-2.34326191 -1.08483259  0.55979629]]\n",
      "net.loss(x+h,t)の結果：\n",
      "0.267830320151\n",
      "x-h\n",
      "[[ 0.88389311  0.19586502  0.35753652]\n",
      " [-2.34326191 -1.08483259  0.55959629]]\n",
      "net.loss(x-h,t)の結果：\n",
      "0.267872616299\n",
      "中心差分による数値微分（偏微分係数）→ (fxh1 - fxh2) / (2*h)：\n",
      "-0.211480737228\n",
      "************勾配結果*************\n",
      "[[ 0.04616536  0.0948218  -0.14098716]\n",
      " [ 0.06924804  0.14223269 -0.21148074]]\n"
     ]
    }
   ],
   "source": [
    "##numerical_gradientの第一引数には関数を渡す必要があるためｆ（W）を定義\n",
    "##Wはnet.Wと同値、ｘは上で定義した[0.6, 0.9]\n",
    "##loss関数の中で、predictとsoftmax（確率っぽい表記に）とcross_entopy_errorを計算して交差エントロピー誤差を返す\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "#####上記の関数再定義は以下のようにも書ける。######\n",
    "#f = lambda w : net.loss(x, t)\n",
    "\n",
    "##上記の損失関数に各重みの値±hを渡して、計算結果の中心差分を取る（＝勾配計算：ここを理解したい）\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(\"************勾配結果*************\")\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "このようにnumerical_gradient(f,x)の結果は、２×３次元の配列になります。勾配の計算結果を見ると、例えばＷ<sub>11</sub>はおよそ0.04ということで、Ｗ<sub>11</sub>をhだけ増やすと損失関数の値は0.04hだけ増加するということを意味します。また、Ｗ<sub>32</sub>は、およそ-0.2ですので、これは損失関数の値を-0.2hだけ減らすというこです。つまり、損失関数を減少するという観点からはＷ<sub>11</sub>はマイナスに、Ｗ<sub>32</sub>はプラスに影響するということです。また更新度合についてもＷ<sub>31</sub>よりもＷ<sub>32</sub>のほうが大きく貢献することもわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行列の偏微分について書きたい"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
